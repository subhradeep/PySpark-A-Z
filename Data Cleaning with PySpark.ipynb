{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Import SQL functions\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "#col package for column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Import UDF function from sql library\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#Random ID generation partitioning wise\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "\n",
    "#Broadcast function Implementation\n",
    "\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_schema = StructType([StructField('name', StringType(),False)\n",
    "                           ,StructField('age', IntegerType(),False)\n",
    "                            ,StructField('city',StringType(),False)\n",
    "                           \n",
    "                           ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the DataFrame for Lazy Processing\n",
    "\n",
    "\n",
    "\n",
    "Lazy Processing in Spark is the idea that very little actually happens until an action is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2017|         0005|                          537|    hnl|\n",
      "|       01/01/2017|         0007|                          498|    ogg|\n",
      "|       01/01/2017|         0037|                          241|    sfo|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aafw_data = spark.read.format('csv').options(Header=True).load('../PySpark/AA_DFW_2017_Departures_Short.csv.gz')\n",
    "aafw_data = aafw_data.withColumn('airport',F.lower(aafw_data['Destination Airport']))\n",
    "aafw_data = aafw_data.drop(aafw_data['Destination Airport'])\n",
    "\n",
    "aafw_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a data format in Parquet format\n",
    "\n",
    "\n",
    "\n",
    "The <b>Parquet</b> format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279962\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('../PySpark/AA_DFW_2017_Departures_Short.csv',header=True)\n",
    "df2 = spark.read.csv('../PySpark/AA_DFW_2016_Departures_Short.csv',header=True)\n",
    "\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "\n",
    "df3 = df3.toPandas()\n",
    "\n",
    "df3.to_parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Please Note:</b>\n",
    "    \n",
    "    \n",
    " There was a technical issue in converting directly to a parquet file which could have been done using the following command:\n",
    "        \n",
    "<b>df3.write.parquet('AA_DFW_ALL.parquet',mode='overwrite')</b>\n",
    "        \n",
    "    \n",
    "    As an alternative the dataframe was converted to a pandas dataframe and then to a parquet file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL and Parquet Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(count(Flight Number)=279962)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df = spark.read.option('header','false').parquet('AA_DFW_ALL.parquet') #reading a parquet file\n",
    "\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "avg_duration = spark.sql('select count(\"Flight Number\") from flights').collect()[0]\n",
    "avg_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Column with PySpark\n",
    "\n",
    "\n",
    "Filtering names which don't resemble to a name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|VOTER_NAME         |\n",
      "+-------------------+\n",
      "|Tennell Atkins     |\n",
      "|Scott Griggs       |\n",
      "|Scott  Griggs      |\n",
      "|Sandy Greyson      |\n",
      "|Michael S. Rawlings|\n",
      "|Kevin Felder       |\n",
      "|Adam Medrano       |\n",
      "|Casey  Thomas      |\n",
      "|011018__42         |\n",
      "|Mark  Clayton      |\n",
      "|Casey Thomas       |\n",
      "|Sandy  Greyson     |\n",
      "|Mark Clayton       |\n",
      "|Jennifer S.  Gates |\n",
      "|Tiffinni A. Young  |\n",
      "|B. Adam  McGough   |\n",
      "|Omar Narvaez       |\n",
      "|Philip T. Kingston |\n",
      "|Rickey D. Callahan |\n",
      "|Dwaine R. Caraway  |\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voters_df = spark.read.csv('../PySpark/DallasCouncilVoters.csv',header=True)\n",
    "\n",
    "#Select voters with distinct voter name\n",
    "voters_df.select(voters_df['VOTER_NAME']).distinct()\n",
    "\n",
    "#Length of voter from 1-20\n",
    "voter_df_filter = voters_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME)<20')\n",
    "\n",
    "#Votername should not contain _ \n",
    "voters_df = voter_df_filter.filter(~F.column('VOTER_NAME').contains('_'))\n",
    "\n",
    "\n",
    "#Voter name \n",
    "voter_df_filter.select('VOTER_NAME').distinct().show(20,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying DataFrames:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Adding columns by splitting the voter_name to first name and last name.\n",
    "    \n",
    "The <b>.getItem(index)</b> takes an integer value to return the appropriately numbered item in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Splits column will look like: \n",
      "\n",
      "+----------+-------------+-----------------+--------------------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|              splits|\n",
      "+----------+-------------+-----------------+--------------------+\n",
      "|02/08/2017|Councilmember|Jennifer S. Gates|[Jennifer, S., Ga...|\n",
      "+----------+-------------+-----------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "\n",
      " \n",
      " After Splitting the column into first name and last name \n",
      "+----------+-------------+-----------------+--------------------+----------+---------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|              splits|first_name|last_name|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+\n",
      "|02/08/2017|Councilmember|Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "voters_df = voters_df.withColumn('splits', F.split(voters_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "print('\\n\\nSplits column will look like: \\n')\n",
    "voters_df.show(1)\n",
    "\n",
    "#Split the column into first and last name using getitem(index) function\n",
    "\n",
    "voters_df = voters_df.withColumn('first_name', voters_df.splits.getItem(0))\n",
    "voters_df = voters_df.withColumn('last_name', voters_df.splits.getItem(F.size('splits') - 1))\n",
    "\n",
    "#Dropping the splits column\n",
    "\n",
    "print('\\n \\n After Splitting the column into first name and last name ')\n",
    "#voters_df = voters_df.drop('splits')\n",
    "voters_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional DataFrame in PySpark:\n",
    "    \n",
    "    \n",
    "    when and otherwise column are the substitute of if-else statement.\n",
    "    \n",
    "    The when() clause lets you conditionally modify a Data Frame based on its content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+\n",
      "|      DATE|        TITLE|       VOTER_NAME|              splits|first_name|last_name|      random_value|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+\n",
      "|02/08/2017|Councilmember|Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.4816924136182461|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voters_df = voters_df.withColumn('random_value',F.when(voters_df.TITLE=='Councilmember',F.rand()))\n",
    "voters_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Multiple When and otherwise usage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|       random_value|         random_val|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.4816924136182461| 0.4241477076323842|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|0.20291972673953607| 0.9266698763601335|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|                2.0|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.08655542731376109|0.05239023055019254|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.03783371418703407|  0.433962731162726|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "voters_df = voters_df.withColumn('random_val',F.when(voters_df.TITLE=='Councilmember',F.rand())\n",
    "                                .when(voters_df.TITLE=='Mayor',2)\n",
    "                                .otherwise(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "voters_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions (UDF)\n",
    "\n",
    "\n",
    "a. The return type from a UDF can be any defined type, even a full StructType() schema object.\n",
    "\n",
    "b. The second argument for UDF can be ArrayType,IntegerType,LongType,StringType etc\n",
    "\n",
    "c. The Second argument for UDF <b>cannot</b> be UDF.\n",
    "\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "|      DATE|        TITLE|         VOTER_NAME|              splits|first_name|last_name|       random_value|         random_val|     getfirstmiddle|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates| 0.4816924136182461| 0.4241477076323842|  Jennifer S. Gates|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|[Philip, T., King...|    Philip| Kingston|0.20291972673953607| 0.9266698763601335| Philip T. Kingston|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|[Michael, S., Raw...|   Michael| Rawlings|               null|                2.0|Michael S. Rawlings|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|     [Adam, Medrano]|      Adam|  Medrano|0.08655542731376109|0.05239023055019254|       Adam Medrano|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|     [Casey, Thomas]|     Casey|   Thomas|0.03783371418703407|  0.433962731162726|       Casey Thomas|\n",
      "+----------+-------------+-------------------+--------------------+----------+---------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voters_df.drop('random_val')\n",
    "\n",
    "\n",
    "#Define a function \n",
    "\n",
    "def getfirstmiddle(names):\n",
    "    \n",
    "    return ' '.join(names)\n",
    "\n",
    "\n",
    "\n",
    "#udf wrapping for the function with Stringtype \n",
    "udffirstmiddle = udf(getfirstmiddle,StringType()) \n",
    "    \n",
    "voters_df = voters_df.withColumn('getfirstmiddle',udffirstmiddle('splits'))\n",
    "\n",
    "voters_df.show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning and Lazy Processing\n",
    "\n",
    "\n",
    "<b>Monotonically increasing IDs :</b> pyspark.sql.functions.monotonically_increasing_id()\n",
    "    \n",
    "    -Integer (64-bit), increases in value, unique\n",
    "    -Not necessarily sequential (gaps exist)\n",
    "    -Completely parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 1 partitions in the voter_df DataFrame.\n",
      "\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+------------------+-----------------+---+\n",
      "|      DATE|        TITLE|       VOTER_NAME|              splits|first_name|last_name|      random_value|        random_val|   getfirstmiddle| ID|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+------------------+-----------------+---+\n",
      "|02/08/2017|Councilmember|Jennifer S. Gates|[Jennifer, S., Ga...|  Jennifer|    Gates|0.4816924136182461|0.4241477076323842|Jennifer S. Gates|  0|\n",
      "+----------+-------------+-----------------+--------------------+----------+---------+------------------+------------------+-----------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding ID field using monotonically increasing ID's\n",
    "\n",
    "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voters_df.rdd.getNumPartitions())\n",
    "voters_df = voters_df.withColumn('ID',monotonically_increasing_id())\n",
    "voters_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID Without Overlapping in Parallel \n",
    "\n",
    "\n",
    "\n",
    "For avoiding overlapping in ID id we can use previous id + F.monotonically_increasing_id()\n",
    "\n",
    "Step 1: Take the previous id using the max() of ID\n",
    "\n",
    "Step 2: Increment the next dataframe id using F.monotincally_increasing_id() + previous_id \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching in PySpark:\n",
    "\n",
    "\n",
    "Caching in Spark refers to storing the result of a DataFrame in memory or on disk of the processing nodes in a cluster. Caching\n",
    "improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source. It reduces the acccess storage,networking and cpu of the Spark as the data is likely already present.\n",
    "\n",
    "Caching is useful if you only plan to use the Dataframe again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_df = spark.read.csv('../PySpark/AA_DFW_2014_Departures_Short.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First call to Count of departure_df 157198 in  0.21143460273742676\n",
      " Second call to Count of departure_df 157198 in  0.1326453685760498\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "departure_cache = departure_df.distinct().cache()\n",
    "print(' First call to Count of departure_df',departure_df.count() ,'in ',time.time()-start_time )\n",
    "\n",
    "start_time= time.time()\n",
    "\n",
    "print(' Second call to Count of departure_df',departure_df.count() ,'in ',time.time()-start_time )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:</b> Though there is a fraction less difference still caching helps  in a cluster mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing a Cache:\n",
    "    \n",
    "    \n",
    "    .unpersist is used for clearing the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Is the dataframe cached ?- True\n",
      "\n",
      " Clearing the Cache ........\n",
      "\n",
      " Is the dataframe cached ?- False\n"
     ]
    }
   ],
   "source": [
    "print('\\n Is the dataframe cached ?-',departure_cache.is_cached)\n",
    "\n",
    "print('\\n Clearing the Cache ........')\n",
    "departure_cache = departure_cache.unpersist()\n",
    "\n",
    "print('\\n Is the dataframe cached ?-',departure_cache.is_cached)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " App name :  pyspark-shell\n",
      "\n",
      " Driver TCP Port: 57597\n",
      "\n",
      " Partitions : 200\n"
     ]
    }
   ],
   "source": [
    "app_name = spark.conf.get('spark.app.name')\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "\n",
    "print('\\n App name : ', app_name)\n",
    "print('\\n Driver TCP Port:',driver_tcp_port )\n",
    "print('\\n Partitions :', partitions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Partitions for a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of Partitions is 2\n",
      "\n",
      "\n",
      " After setting partition manually 2\n"
     ]
    }
   ],
   "source": [
    "before = departure_df.rdd.getNumPartitions()\n",
    "print('\\n Number of Partitions is',before)\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 1)\n",
    "\n",
    "after = spark.read.csv('../PySpark/AA_DFW_2014_Departures_Short.csv',header=True)\n",
    "\n",
    "print('\\n\\n After setting partition manually',departure_df.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_df = spark.read.csv('../PySpark/airports.csv',header=True)\n",
    "flights_df = spark.read.csv('../PySpark/flights_small.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain is as below:\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [faa#460], [dest#495], Inner, BuildLeft\n",
      ":- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
      ":  +- *(1) Project [faa#460, name#461, lat#462, lon#463, alt#464, tz#465, dst#466]\n",
      ":     +- *(1) Filter isnotnull(faa#460)\n",
      ":        +- *(1) FileScan csv [faa#460,name#461,lat#462,lon#463,alt#464,tz#465,dst#466] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/subhr/PySpark/airports.csv], PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "+- *(2) Project [year#484, month#485, day#486, dep_time#487, dep_delay#488, arr_time#489, arr_delay#490, carrier#491, tailnum#492, flight#493, origin#494, dest#495, air_time#496, distance#497, hour#498, minute#499]\n",
      "   +- *(2) Filter isnotnull(dest#495)\n",
      "      +- *(2) FileScan csv [year#484,month#485,day#486,dep_time#487,dep_delay#488,arr_time#489,arr_delay#490,carrier#491,tailnum#492,flight#493,origin#494,dest#495,air_time#496,distance#497,hour#498,minute#499] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/subhr/PySpark/flights_small.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n"
     ]
    }
   ],
   "source": [
    "join_df = airports_df.join(flights_df,airports_df['faa']==flights_df['dest'])\n",
    "\n",
    "print('Explain is as below:\\n\\n')\n",
    "join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Join with Explain Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain Plan for Broadcast is as below:\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [faa#460], [dest#495], Inner, BuildRight\n",
      ":- *(2) Project [faa#460, name#461, lat#462, lon#463, alt#464, tz#465, dst#466]\n",
      ":  +- *(2) Filter isnotnull(faa#460)\n",
      ":     +- *(2) FileScan csv [faa#460,name#461,lat#462,lon#463,alt#464,tz#465,dst#466] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/subhr/PySpark/airports.csv], PartitionFilters: [], PushedFilters: [IsNotNull(faa)], ReadSchema: struct<faa:string,name:string,lat:string,lon:string,alt:string,tz:string,dst:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[11, string, true]))\n",
      "   +- *(1) Project [year#484, month#485, day#486, dep_time#487, dep_delay#488, arr_time#489, arr_delay#490, carrier#491, tailnum#492, flight#493, origin#494, dest#495, air_time#496, distance#497, hour#498, minute#499]\n",
      "      +- *(1) Filter isnotnull(dest#495)\n",
      "         +- *(1) FileScan csv [year#484,month#485,day#486,dep_time#487,dep_delay#488,arr_time#489,arr_delay#490,carrier#491,tailnum#492,flight#493,origin#494,dest#495,air_time#496,distance#497,hour#498,minute#499] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/subhr/PySpark/flights_small.csv], PartitionFilters: [], PushedFilters: [IsNotNull(dest)], ReadSchema: struct<year:string,month:string,day:string,dep_time:string,dep_delay:string,arr_time:string,arr_d...\n"
     ]
    }
   ],
   "source": [
    "normal_df2 = airports_df.join(broadcast(flights_df),airports_df['faa']==flights_df['dest'])\n",
    "\n",
    "print('Explain Plan for Broadcast is as below:\\n\\n')\n",
    "normal_df2.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal join takes  0.2403581142425537\n",
      "Broadcast takes  0.13164687156677246\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "join_count = join_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time2 = time.time()\n",
    "\n",
    "broadcast_2 = normal_df2.count()\n",
    "broadcast_duration = time.time()-start_time2\n",
    "\n",
    "print('Normal join takes ',normal_duration)\n",
    "print('Broadcast takes ',broadcast_duration)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures_df2 = spark.read.csv('../PySpark/AA_DFW_2015_Departures_Short.csv',header=True)\n",
    "\n",
    "#Filter Actual elapsed time not to include 0 duration\n",
    "departures_df2 = departures_df2.filter(departures_df2[3]==0)\n",
    "\n",
    "#Adding a column\n",
    "departures_df2 = departures_df2.withColumn('id',F.monotonically_increasing_id())\n",
    "\n",
    "#Exporting the data to a JSON file\n",
    "\n",
    "#departures_df2.write.json('departure.json',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column with .STARTSWITH function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Destination Airport|Actual elapsed time (Minutes)| id|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "|       01/01/2015|         0194|                ATL|                            0|  5|\n",
      "|       01/01/2015|         1023|                AUS|                            0| 13|\n",
      "|       01/01/2015|         1484|                AUS|                            0| 27|\n",
      "|       01/01/2015|         1662|                ABQ|                            0| 33|\n",
      "|       01/02/2015|         1457|                AUS|                            0| 62|\n",
      "+-----------------+-------------+-------------------+-----------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "departures_df2.where(col('Destination Airport').startswith('A')).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
